{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"8n7zoah5p2ku"},"source":[" *Artificial Intelligence for Vision & NLP* &nbsp; | &nbsp;  *ATU Donegal - MSc in Big Data Analytics & Artificial Intelligence*\n","\n","#Rule and Phrase Matching\n","\n","So far we've seen how text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.\n","\n","In this section we will identify and label specific tokens and phrases that match patterns we can define ourselves. "]},{"cell_type":"markdown","metadata":{"id":"xTQPGj1ip2kx"},"source":["## Rules-based Matching\n","\n","spaCy’s rule-based matcher engines and components not only let you find you the words and phrases you’re looking for – they also give you access to the tokens within the document and their relationships. This means you can easily access and analyse the surrounding tokens, merge spans into single tokens or add entries to the named entities in `doc.ents`.\n","\n","spaCy offers a *rule-matching tool* called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. \n","\n","We can match on any part of the token including text and annotations, and web add multiple patterns to the same matcher."]},{"cell_type":"code","metadata":{"id":"sbRNUjOBp2kz"},"source":["# Perform standard imports\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCY3W0lIp2kz"},"source":["## Creating a token pattern\n","\n","For this example, suppose we want to find three combinations of the words *stop word*. The three combinations of these words are:\n","\n","(a) a token that looks for lowercase text *stopword*<br>\n","(b) a token where the `is_punct` flag is set to `True` so that any punctuation is detected eg *stop-word*<br>\n","(c) a token where two words are found that read *stop* and *word* with a space in between eg *stop word*<br>"]},{"cell_type":"markdown","metadata":{"id":"Lc4Lj1stp2k0"},"source":["First we import the `Matcher` library:"]},{"cell_type":"code","metadata":{"id":"-_pT8Az9p2k0"},"source":["# Import the Matcher library\n","from spacy.matcher import Matcher\n","matcher = Matcher(nlp.vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qmgqZeT3p2k1"},"source":["Then we create each pattern. There are several token attributes we can use. These are shown below."]},{"cell_type":"markdown","metadata":{"id":"NbVq8_b8p2k1"},"source":["\n","\n","<thead><tr class=\"_8a68569b\"><th class=\"_2e8d2972\">Attribute</th><th class=\"_2e8d2972\">Type</th><th class=\"_2e8d2972\">&nbsp;Description</th></tr></thead>\n","<tbody><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">ORTH</code></td><td class=\"_5c99da9a\">unicode</td><td class=\"_5c99da9a\">The exact verbatim text of a token.</td>\n","    </tr>\n","    <tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">LOWER</code></td><td class=\"_5c99da9a\">unicode</td><td class=\"_5c99da9a\">The lowercase form of the token text.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">LENGTH</code></td><td class=\"_5c99da9a\">int</td><td class=\"_5c99da9a\">The length of the token text.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">IS_ALPHA</code>, <code class=\"_1d7c6046\">IS_ASCII</code>, <code class=\"_1d7c6046\">IS_DIGIT</code></td><td class=\"_5c99da9a\">bool</td><td class=\"_5c99da9a\">Token text consists of alphabetic characters, ASCII characters, digits.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">IS_LOWER</code>, <code class=\"_1d7c6046\">IS_UPPER</code>, <code class=\"_1d7c6046\">IS_TITLE</code></td><td class=\"_5c99da9a\">bool</td><td class=\"_5c99da9a\">Token text is in lowercase, uppercase, titlecase.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">IS_PUNCT</code>, <code class=\"_1d7c6046\">IS_SPACE</code>, <code class=\"_1d7c6046\">IS_STOP</code></td><td class=\"_5c99da9a\">bool</td><td class=\"_5c99da9a\">Token is punctuation, whitespace, stop word.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">LIKE_NUM</code>, <code class=\"_1d7c6046\">LIKE_URL</code>, <code class=\"_1d7c6046\">LIKE_EMAIL</code></td><td class=\"_5c99da9a\">bool</td><td class=\"_5c99da9a\">Token text resembles a number, URL, email.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">POS</code>, <code class=\"_1d7c6046\">TAG</code>, <code class=\"_1d7c6046\">DEP</code>, <code class=\"_1d7c6046\">LEMMA</code>, <code class=\"_1d7c6046\">SHAPE</code></td><td class=\"_5c99da9a\">unicode</td><td class=\"_5c99da9a\">The token’s simple and extended part-of-speech tag, dependency label, lemma, shape.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">ENT_TYPE</code></td><td class=\"_5c99da9a\">unicode</td><td class=\"_5c99da9a\">The token’s entity label.</td></tr>\n","    </tbody>"]},{"cell_type":"markdown","metadata":{"id":"whv0zK-Hp2k2"},"source":["Here's the three matching tokens for the three combinations of *stop word* described above. Note that we don't need to tokenise a single space as it is not recognised as punctuation.\n","\n","It doesn't matter if the attribute names are upper or lowercase. spaCy will normalise the names internally and `{\"LOWER\": \"text\"}` and `{\"lower\": \"text\"}` will both produce the same result. Using the uppercase version is mostly a convention to make it clear that the attributes are *special* and don’t exactly map to the token attributes like `Token.lower` and `Token.lower_`."]},{"cell_type":"code","metadata":{"id":"4-zYMIGYp2k3"},"source":["# match for \"stopword\"\n","token_match1 = [{\"LOWER\": \"stopword\"}]\n","# match for \"stopwords\"\n","token_match2 = [{\"LOWER\": \"stopwords\"}]\n","# match for stop-word\n","token_match3 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"word\"}]\n","# match for stop-words\n","token_match4 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"words\"}]\n","# match for \"stop word\". We don't need to check for a single space as it is not tokenised\n","token_match5 = [{\"LOWER\": \"stop\"}, {\"LOWER\": \"word\"}]\n","# stopwords\n","token_match6 = [{\"LOWER\": \"stop\"}, {\"LOWER\": \"words\"}]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KZ7Kcbirp2k3"},"source":["Then we call `matcher.add` command to add all three token matches. The second argument lets you pass in an optional callback function to invoke on a successful match. For now, we set it to `None`."]},{"cell_type":"code","metadata":{"id":"jtXF0UW3p2k3"},"source":["matcher.add(\"StopWord\", [token_match1, token_match2, token_match3, token_match4, token_match5, token_match6])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MlmR3kJ-p2k4"},"source":["## Applying the matcher to a doc object\n"]},{"cell_type":"code","metadata":{"id":"MWKRlLLCp2k4"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","file_name = open(\"/content/gdrive/My Drive/NLP/stopwords.txt\")\n","sentence = file_name.read()\n","doc_object = nlp(sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHGVIk8Ap2k4"},"source":["print(doc_object)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVROycq2p2k5"},"source":["token_matches = matcher(doc_object)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VwCxANFSp2k6"},"source":["for token in token_matches:\n","    print(token)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLMFJTVjp2k7"},"source":["Lets create a function that accepts a string and displays the matcher objects. We'll also structure the output of the function."]},{"cell_type":"code","metadata":{"id":"hlsyM52Kp2k7"},"source":["def find_matches(text):\n","    # convert text to a doc object\n","    doc_object = nlp(text)\n","    print(doc_object)\n","    # find all matches within the doc object\n","    token_matches = matcher(doc_object)\n","    # For each item in the token_matches provide the following\n","    # match_id is the hash value of the identified token match\n","    for match_id, start, end in token_matches:\n","        string_id = nlp.vocab.strings[match_id]\n","        matched_span = doc_object[start:end]      \n","        print(f\"{match_id:<{20}} {string_id:<{15}} {start:{3}} {end:{3}} {matched_span.text:{20}}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l4Tj24a9p2k7"},"source":["Now we'll pass in the text from the earlier example into the function."]},{"cell_type":"code","metadata":{"id":"JUrCfiHmp2k8"},"source":["find_matches(sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YAaZebGXp2k8"},"source":["### Setting pattern options and quantifiers\n","\n","The following quantifiers can be passed to the `'OP'` key:\n","<table><tr><th>OP</th><th>Description</th></tr>\n","\n","<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n","<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n","<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n","<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n","</table>\n","\n","You can make token rules optional by passing an `'OP':'*'` argument.  \n","\n","This lets us streamline our patterns list:"]},{"cell_type":"code","source":["# Remove old matcher to avoid issues\n","matcher.remove(\"StopWord\")"],"metadata":{"id":"88iiV9YEkwBO"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__HGfmdip2k9"},"source":["# Redefine the patterns:\n","token_match1 = [{\"LOWER\": \"stopword\"}]\n","token_match2 = [{\"LOWER\": \"stopwords\"}]\n","token_match3 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True, \"OP\":\"*\"}, {\"LOWER\": \"word\"}]\n","token_match4 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True, \"OP\":\"*\"}, {\"LOWER\": \"words\"}]\n","token_match5 = [{\"LOWER\": \"stop\"}, {\"LOWER\": \"word\"}]\n","token_match6 = [{\"LOWER\": \"stop\"}, {\"LOWER\": \"words\"}]\n","\n","# Add the new set of patterns to the 'SolarPower' matcher:\n","matcher.add(\"StopWord\", [token_match1, token_match2, token_match3, token_match4, token_match5, token_match6])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wazGrFJ7p2k9"},"source":["file_name = open(\"/content/gdrive/My Drive/NLP/stopwords.txt\")\n","sentence = file_name.read()\n","#my_text = \"Words like \\\"a\\\" and \\\"the\\\" are called stop---words.\\\n","#Sometimes this can be written as stop-words or stopwords.\\\n","#Each stop word can be filtered from the text to be processed.\\\n","#spaCy holds a built-in list of some 305 English stop--words.\"\n","\n","find_matches(sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ycYl5zBOp2k-"},"source":["## Be careful with Lemmatisation Searching\n","If we wanted to match on the words *petrol power* and *petrol powered*, it might be tempting to look for the *lemma* of *powered* and expect it to be *power*. Then we could potentially pick that up with a *lemmatisation* match. This is not always the case though: the lemma of the adjective *powered* is still *powered*.\n","\n","Lets look at an example of this problem. First we'll create a sample sentence and show the lemmas from it."]},{"cell_type":"code","metadata":{"id":"lJZne67Ip2k-"},"source":["doc_object = nlp(u\"Petrol powered energy runs petrol powered cars.\")\n","\n","# Lets look at the lemmatisation of each word\n","for word in doc_object:\n","    print (word.text + \"\\t\" + \" -----> \" + word.lemma_ + \"\\t\" + word.pos_ + \"\\t\" + word.tag_ + \"\\t\" + spacy.explain(word.tag_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc_object = nlp(u\"Petrol powered cars run on petrol powered energy.\")\n","\n","# Lets look at the lemmatisation of each word\n","for word in doc_object:\n","    print (word.text + \"\\t\" + \" -----> \" + word.lemma_ + \"\\t\" + word.pos_ + \"\\t\" + word.tag_ + \"\\t\" + spacy.explain(word.tag_))"],"metadata":{"id":"cqd2GXy06DPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDSd_xQcp2k-"},"source":["The first occurrence of *powered* is an adjective so it can't match on the lemma *power* since an adjective does not reduce down to the base word *power*. This example will not work as expected."]},{"cell_type":"code","metadata":{"id":"qKbxhnEGp2k-"},"source":["token_match1 = [{'LOWER': 'petrolpower'}]\n","token_match2 = [{'LOWER': 'petrol'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}]\n","\n","# Add the new set of patterns to the 'SolarPower' matcher:\n","matcher.add('PetrolPower', [token_match1, token_match2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ulCM9cWLp2k_"},"source":["found_matches = matcher(doc_object)\n","print (found_matches)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwbOmyM7p2k_"},"source":["Only the second occurrence of *petrol powered* is recognised. The first occurrence's lemma equivelant does not change to *power* so it is not matched."]},{"cell_type":"markdown","metadata":{"id":"28y5I8vnp2k_"},"source":["# Phrase Matcher\n","In token-based matching we used token patterns to perform rule-based matching. \n","\n","An alternative - and often more efficient method is to match on terminology lists. In this case we use `PhraseMatcher` to create a Doc object from a list of phrases, and pass that into `matcher` instead."]},{"cell_type":"code","metadata":{"id":"YVxgakusp2k_"},"source":["# Import the PhraseMatcher library\n","from spacy.matcher import PhraseMatcher\n","matcher = PhraseMatcher(nlp.vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RkKEK7Xop2lA"},"source":["The example text is from https://en.wikipedia.org/wiki/Natural_language_processing\n","    \n","It is also available on Blackboard as `NLP.txt`."]},{"cell_type":"code","metadata":{"id":"Dy683Yoip2lA"},"source":["with open(\"/content/gdrive/My Drive/NLP/NLP.txt\", encoding = \"utf8\") as my_file:\n","    doc_object = nlp(my_file.read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ySVCfBImp2lA"},"source":["Now we want to match on some words within the text file we've just imported. Let's create a list of match phrases we'd like to check the imported text for:"]},{"cell_type":"code","metadata":{"id":"7HKcasXMp2lA"},"source":["phrase_list = [\"natural language processing\", \"machine learning\", \"supervised learning\", \"machine translation\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zmIs4n9hp2lA"},"source":["Next we convert each of these phrases into a suitable structure. I'm going to create a `doc` object."]},{"cell_type":"code","metadata":{"id":"lQ6inxKLp2lA"},"source":["# Next, convert each phrase to a Doc object:\n","phrase_patterns = [nlp.make_doc(word) for word in phrase_list]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jQt-z9PJp2lB"},"source":["Lets have a look at these phrase patterns."]},{"cell_type":"code","metadata":{"id":"Lm8Q-yTpp2lB"},"source":["# Show these phrase patterns\n","print(phrase_patterns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1L9-rbYIp2lB"},"source":["Now we can add each of these phrase patterns to a `matcher` object called `NLP`."]},{"cell_type":"code","metadata":{"id":"bOpJSDxGp2lB"},"source":["# Pass each Doc object into matcher (note the use of the asterisk)\n","# refers to a *phrase_patterns (Doc): `Doc` objects representing match patterns.\n","matcher.add(\"NLP\", None, *phrase_patterns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jj8pEeeap2lB"},"source":["Finally we build a list of relevant matches and put the results into a variable called `matches`."]},{"cell_type":"code","metadata":{"id":"EBQUzbpEp2lB"},"source":["# Build a list of matches:\n","matches = matcher(doc_object)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FlznWxHzp2lC"},"source":["Lets have a look at the contents of the found matches. Each match contains the `match_id`, and the `start` and `stop` locations of each match within the text file."]},{"cell_type":"code","metadata":{"id":"yEEv_Ja4p2lC"},"source":["matches"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"imHkygbyp2lC"},"source":["We can show each match using a loop we created earlier in this document. "]},{"cell_type":"code","metadata":{"id":"V1XuxlX3p2lC"},"source":["for match_id, start, end in matches:\n","    string_id = nlp.vocab.strings[match_id]\n","    span = doc_object[start:end]\n","    print(match_id, \"\\t\", string_id, \"\\t\", start, \"\\t\", end, \"\\t\", span.text)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwqSldeDp2lC"},"source":["## Viewing Matches\n","There are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc object that is wider than the match.\n","\n","For example, the first occurrence of 'machine translation' occurs between words 85 - 86. We can view the context of the sentence it is in by choosing a few words either side of its location within the string."]},{"cell_type":"code","metadata":{"id":"V4TFQBIBp2lD"},"source":["# Allowing a few words either side of the match\n","doc_object[80:93]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Et1FBfZap2lD"},"source":["We could use the loop we created earlier to capture some text on either side of the matched phrase."]},{"cell_type":"code","metadata":{"id":"KEcaLFRgp2lD"},"source":["for match_id, start, end in matches:\n","    string_id = nlp.vocab.strings[match_id]\n","    span = doc_object[start-3:end+3]\n","    print(string_id, \"\\t\", start, \"\\t\", end, \"\\t\", span.text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rHlRUaWgp2lD"},"source":["Another way is to first apply the `sentencizer` to the doc object, then iterate through the sentences to the match point:"]},{"cell_type":"code","metadata":{"id":"JvaioFy2p2lD"},"source":["# Build a list of sentences\n","sentences = [sent for sent in doc_object.sents]\n","\n","# Sentences contain start and end token values\n","# for example, here's the start and end values of the first sentence\n","print(sentences[0].start, sentences[0].end)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jQAlUaoep2lE"},"source":["# Iterate over the sentence list until the sentence end value exceeds a match start value:\n","for sent in sentences:\n","    # matches[2][2] refers to the 3rd row in matches and the third column \"129\"\n","    # send.end is the end of an occurrence of \"sent\"\n","    if matches[2][2] < sent.end:\n","        print(sent, sent.start, sent.end, matches[2][2])\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise\n","\n","For the paragraph of text below, write a pattern that matches a form of \"download\" plus a proper noun. Add the pattern to the matcher and print the matches."],"metadata":{"id":"4YmEI0_vxkBi"}},{"cell_type":"code","source":["import spacy\n","from spacy.matcher import Matcher\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","matcher = Matcher(nlp.vocab)\n","\n","doc = nlp(\n","    \"i downloaded Minecraft on my PC and can't open it. Can you help? \"\n","    \"When I was downloading the game, I got the Windows version in a \"\n","    \"'.zip' folder and I used the default program to unpack it... do \"\n","    \"I also need to download WinZip?\"\n",")\n","\n","pattern = []\n"],"metadata":{"id":"DR3_J8V7xl6Z"},"execution_count":null,"outputs":[]}]}