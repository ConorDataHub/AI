{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Object"
      ],
      "metadata": {
        "id": "lEP1ld78BFJI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkLN4gRi_ld5",
        "outputId": "6e0af7c2-e9d1-42cb-ecde-e1fb442845fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a sample sentence.', 'This is another sentence.', 'And this is a third sentence.']\n"
          ]
        }
      ],
      "source": [
        "#    Q1. Create an NLP document object \n",
        "#    Then using a loop, store all the sentences of the document object into an array\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a doc object with your text\n",
        "doc = nlp(\"This is a sample sentence. This is another sentence. And this is a third sentence.\")\n",
        "\n",
        "# Create an empty list to store the sentences\n",
        "sentences = []\n",
        "\n",
        "# Loop through each sentence in the doc object and append it to the list\n",
        "for sent in doc.sents:\n",
        "    sentences.append(sent.text)\n",
        "\n",
        "# Print the list of sentences\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation, POS Tagging, Dependency"
      ],
      "metadata": {
        "id": "RumfHZWoBDmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from your text document and show the following information using an f-string:\n",
        "# (a) Tokenisation is the process of breaking down a piece of text into smaller pieces of text.\n",
        "# (b) Token POS tag is the process of labeling each token in a text with its corresponding part of speech.\n",
        "# (c) Token dependency is the process of identifying the dependencies between words in a sentence, such as subject-verb or object-preposition relationships.\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a doc object with your text\n",
        "doc = nlp(\"The cat sat on the mat.\")\n",
        "\n",
        "# Loop through each token in the doc object and print its text, POS tag, and dependency\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, POS tag: {token.pos_}, Dependency: {token.dep_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aRW3Eav_2vo",
        "outputId": "c9646ede-afe7-4fb9-a8a5-d2cc3677ba8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: The, POS tag: DET, Dependency: det\n",
            "Token: cat, POS tag: NOUN, Dependency: nsubj\n",
            "Token: sat, POS tag: VERB, Dependency: ROOT\n",
            "Token: on, POS tag: ADP, Dependency: prep\n",
            "Token: the, POS tag: DET, Dependency: det\n",
            "Token: mat, POS tag: NOUN, Dependency: pobj\n",
            "Token: ., POS tag: PUNCT, Dependency: punct\n"
          ]
        }
      ]
    }
  ]
}